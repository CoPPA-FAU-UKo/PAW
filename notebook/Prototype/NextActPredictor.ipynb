{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5ce5b24-07e9-49e3-a034-cac4c30eae4a",
   "metadata": {},
   "source": [
    "# Remaining time predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "817711e5-a662-4ff5-be3f-7a393a14920c",
   "metadata": {},
   "source": [
    "import xgboost as xgb\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from joblib import dump, load\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import importlib.util\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from src.Trainer import CaseDataSet\n",
    "from src.Trainer import NextActPredictor\n",
    "from src.Model import DLModels\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import f1_score\n",
    "    \n",
    "\n",
    "\n",
    "torch_device = \"cpu\"\n",
    "device_package = torch.cpu\n",
    "if importlib.util.find_spec(\"torch.backends.mps\") is not None:\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch_device = torch.device(\"mps\")\n",
    "        device_package = torch.mps\n",
    "if torch.cuda.is_available():\n",
    "    torch_device = torch.device(\"cuda\")\n",
    "    device_package = torch.cuda\n",
    "    \n",
    "torch_device"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3df3e5a8-b7c5-4bdf-86a5-7589db8cbaff",
   "metadata": {},
   "source": [
    "class XGBTrainer():\n",
    "    def __init__(self, training_set, validation_set, num_class, model_path, tree_method=\"hist\", early_stopping_rounds=2):\n",
    "        self.training_set = training_set\n",
    "        self.validation_set = validation_set\n",
    "        self.model_path = model_path\n",
    "        self.dataset_list = [self.training_set, self.validation_set]\n",
    "        self.data_list = []\n",
    "        self.clf = xgb.XGBClassifier(objective='multi:softprob', tree_method=tree_method, early_stopping_rounds=early_stopping_rounds, num_class=num_class)\n",
    "        self.le = preprocessing.LabelEncoder()\n",
    "        self.generate_data_set()\n",
    "        \n",
    "    def generate_data_set(self):\n",
    "       \n",
    "        for i in range(2):\n",
    "            feature_list = []\n",
    "            label_list = []\n",
    "            for prefix_len in range(1, self.dataset_list[i].max_case_len-1):\n",
    "                self.dataset_list[i].set_prefix_length(prefix_len)\n",
    "                feature_list.append(self.dataset_list[i][:][0].numpy())\n",
    "                label_list.append(self.dataset_list[i][:][1].numpy())\n",
    "        \n",
    "            self.data_list.append([np.vstack(feature_list), np.argmax(np.vstack(label_list), axis=-1)])\n",
    "            if i == 0:\n",
    "                self.le.fit(self.data_list[0][1])\n",
    "                \n",
    "            self.data_list[i][1] = self.le.transform(self.data_list[i][1])\n",
    "            \n",
    "                   \n",
    "    \n",
    "    def train(self):\n",
    "        self.clf.fit(self.data_list[0][0], self.data_list[0][1], eval_set=[(self.data_list[1][0], self.data_list[1][1])])\n",
    "        \n",
    "    def score(self):\n",
    "        return self.clf.score(self.data_list[1][0], self.data_list[1][1])\n",
    "    \n",
    "    def save_model(self):\n",
    "        self.clf.save_model(self.model_path + \"_XGB.json\")\n",
    "        dump(self.le, self.model_path + \"_le.joblib\")\n",
    "        \n",
    "    def load_model(self):\n",
    "        self.clf.load_model(self.model_path + \"_XGB.json\")\n",
    "        self.le = load(self.model_path + \"_le.joblib\")\n",
    "        \n",
    "\n",
    "class XGBPredictor():\n",
    "    def __init__(self, test_set, num_class, model_path, tree_method=\"hist\", early_stopping_rounds=2):\n",
    "        self.test_set = test_set\n",
    "        self.model_path = model_path\n",
    "        self.data_list = []\n",
    "        self.clf = xgb.XGBClassifier(objective='multi:softprob', tree_method=tree_method, early_stopping_rounds=early_stopping_rounds, num_class=num_class)\n",
    "        self.clf.load_model(self.model_path + \"_XGB.json\")\n",
    "        self.le = load(self.model_path + \"_le.joblib\")\n",
    "        self.generate_data_set()\n",
    "    \n",
    "    def generate_data_set(self):\n",
    "        feature_list = []\n",
    "        label_list = []\n",
    "        for prefix_len in range(1, self.test_set.max_case_len-1):\n",
    "            self.test_set.set_prefix_length(prefix_len)\n",
    "            feature_list.append(self.test_set[:][0].numpy())\n",
    "            label_list.append(self.test_set[:][1].numpy())\n",
    "\n",
    "        self.data_list = [np.vstack(feature_list), np.argmax(np.vstack(label_list), axis=-1)]\n",
    "        self.data_list[1] = self.le.transform(self.data_list[1])\n",
    "        \n",
    "    def predict(self):\n",
    "        return self.clf.predict(self.data_list[0]), self.data_list[1]\n",
    "        \n",
    "        \n",
    "        "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aec5882e-557b-43f6-a9bb-d6b0144bec0b",
   "metadata": {},
   "source": [
    "encoding = \"Agg_Mean\"\n",
    "train = CaseDataSet.CaseDataset(project_data_path=\"../../data/\", input_data=\"dummy\", data_version=\"_train\", feature_list=[\"Activity\", \"LapseTime\"],\n",
    "                                encoding=encoding, label=\"Next_Activity\")\n",
    "\n",
    "val = CaseDataSet.CaseDataset(project_data_path=\"../../data/\", input_data=\"dummy\", data_version=\"_val\", feature_list=[\"Activity\", \"LapseTime\"],\n",
    "                              encoding=encoding, label=\"Next_Activity\")\n",
    "\n",
    "test = CaseDataSet.CaseDataset(project_data_path=\"../../data/\", input_data=\"dummy\", data_version=\"_test\", feature_list=[\"Activity\", \"LapseTime\"],\n",
    "                              encoding=encoding, label=\"Next_Activity\")\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6c03835-4b4d-4d8b-879a-e4aab0162fd5",
   "metadata": {},
   "source": [
    "t1 = XGBTrainer(train, val, model_path=\"../../models/test\", num_class=6)\n",
    "t1.train()\n",
    "print(t1.score())\n",
    "t1.save_model()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7d96925b-1d1d-4350-a73c-812a2ce1fcdd",
   "metadata": {},
   "source": [
    "t2 = XGBPredictor(test, model_path=\"../../models/test\", num_class=6)\n",
    "t2.predict()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a11676bd-e820-4893-8322-5422c1bc2e3c",
   "metadata": {},
   "source": [
    "t2.data_list[1]"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "bc7dc0d7-0cf8-45a7-9721-c8e1f3fc536e",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ac588fa1-ef3c-4c97-9b64-cbb459ee9ea2",
   "metadata": {},
   "source": [
    "def train_model_epoch(model, training_set, optimizer, criterion, torch_device,\n",
    "                      batch_size=50, training=True):\n",
    "    training_data_set = training_set\n",
    "    batch_size = batch_size\n",
    "    loss_prefix_list = []\n",
    "    sample_num_list = []\n",
    "    for prefix_len in range(1, training_data_set.max_case_len - 1):\n",
    "        loss_prefix = 0\n",
    "        training_data_set.set_prefix_length(prefix_len)\n",
    "        training_data_set.shuffle_data()\n",
    "        input_data = training_data_set[:]\n",
    "        if input_data is None:\n",
    "            break\n",
    "        sample_num = input_data[0].shape[0]\n",
    "        sample_num_list.append(sample_num)\n",
    "\n",
    "        batch_num = int(sample_num / batch_size)\n",
    "        for i in range(batch_num):\n",
    "            x = input_data[0][int(batch_size * i) : int(batch_size * (i+1))].float().to(torch_device)\n",
    "            y = input_data[1][int(batch_size * i) : int(batch_size * (i+1))].float().to(torch_device).argmax(dim=1)\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            if training:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            loss_prefix = loss_prefix + loss.item()\n",
    "\n",
    "        if sample_num > batch_size * batch_num:\n",
    "            x = input_data[0][batch_size * batch_num :].float().to(torch_device)\n",
    "            y = input_data[1][batch_size * batch_num :].float().to(torch_device).argmax(dim=1)\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            if training:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            loss_prefix = loss_prefix + loss.item()\n",
    "\n",
    "        loss_prefix_list.append(loss_prefix)\n",
    "    return np.array(loss_prefix_list), np.array(sample_num_list)\n",
    "\n",
    "\n",
    "def train_model(model, optimizer, criterion, criterion_eval, training_set,\n",
    "                test_set, batch_size, torch_device, device_package,\n",
    "                max_epoch=100, max_ob_iter=20, score_margin=1e-4, print_iter=False):\n",
    "    train_score_list = []\n",
    "    test_score_list = []\n",
    "    score = 1e5\n",
    "    best_iter = 0\n",
    "    best_model = None\n",
    "    for iter_epoch in range(max_epoch):\n",
    "        device_package.empty_cache()\n",
    "        loss_train, sample_num_train = train_model_epoch(model, training_set, batch_size=batch_size,\n",
    "                                                                 optimizer=optimizer,\n",
    "                                                                 criterion=criterion,\n",
    "                                                                 torch_device=torch_device)\n",
    "        device_package.empty_cache()\n",
    "        loss_test, sample_num_test = train_model_epoch(model, test_set, batch_size=batch_size,\n",
    "                                                               optimizer=optimizer,\n",
    "                                                               criterion=criterion_eval,\n",
    "                                                               torch_device=torch_device,\n",
    "                                                               training=False)\n",
    "\n",
    "        score_train = np.sum(loss_train) / np.sum(sample_num_train)\n",
    "        score_test = np.sum(loss_test) / np.sum(sample_num_test)\n",
    "        train_score_list.append(score_train)\n",
    "        test_score_list.append(score_test)\n",
    "\n",
    "        if score_test < (score - score_margin):\n",
    "            score = score_test\n",
    "            best_model = copy.deepcopy(model)\n",
    "            best_iter = iter_epoch\n",
    "\n",
    "        if iter_epoch > best_iter + max_ob_iter:\n",
    "            break\n",
    "        if print_iter:\n",
    "            print(\"Finished training iteration: \", iter_epoch, \" with val loss: \", score_test)\n",
    "    device_package.empty_cache()\n",
    "    return best_model, np.array(train_score_list), np.array(test_score_list)\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_set, torch_device, device_package, batch_size=100):\n",
    "    training_data_set = test_set\n",
    "    evaluation_list = []\n",
    "    sample_num_list = []\n",
    "    model.flatten()\n",
    "    device_package.empty_cache()\n",
    "    for prefix_len in range(training_data_set.max_case_len - 1):\n",
    "        training_data_set.set_prefix_length(prefix_len + 1)\n",
    "        training_data_set.shuffle_data()\n",
    "        input_data = training_data_set[:]\n",
    "        if input_data is None:\n",
    "            # print(\"Max length reached, abort\")\n",
    "            break\n",
    "        sample_num = input_data[0].shape[0]\n",
    "        sample_num_list.append(sample_num)\n",
    "        \n",
    "        output_list = []\n",
    "        label_list = []\n",
    "        batch_num = int(sample_num / batch_size)\n",
    "        for i in range(batch_num):\n",
    "            x = input_data[0][int(batch_size * i) : int(batch_size * (i+1))].float().to(torch_device)\n",
    "            y = input_data[1][int(batch_size * i) : int(batch_size * (i+1))].float()\n",
    "            outputs = model(x).detach()\n",
    "            prob = F.softmax(outputs, dim=-1)\n",
    "            output_list.append(prob.cpu().numpy())\n",
    "            label_list.append(y.cpu().numpy())\n",
    "            \n",
    "            device_package.empty_cache()\n",
    "\n",
    "        if sample_num > batch_size * batch_num:\n",
    "            x = input_data[0][batch_size * batch_num :].float().to(torch_device)\n",
    "            y = input_data[1][batch_size * batch_num :].float()\n",
    "            outputs = model(x).detach()\n",
    "            prob = F.softmax(outputs, dim=-1)\n",
    "            output_list.append(prob.cpu().numpy())\n",
    "            label_list.append(y.cpu().numpy())\n",
    "            \n",
    "            device_package.empty_cache()\n",
    "        \n",
    "        evaluation_list.append([np.vstack(output_list),\n",
    "                                np.vstack(label_list)])\n",
    "        \n",
    "        \n",
    "    return evaluation_list, np.array(sample_num_list)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f3945da-fe57-4f51-9dd9-ddd52c0c54ff",
   "metadata": {},
   "source": [
    "encoding = \"All\"\n",
    "train = CaseDataSet.CaseDataset(project_data_path=\"../../data/\", input_data=\"test1\", data_version=\"\",\n",
    "                                    feature_list=[\"Activity\", \"LapseTime\"],\n",
    "                                    encoding=encoding, label=\"Next_Activity\")\n",
    "\n",
    "test = CaseDataSet.CaseDataset(project_data_path=\"../../data/\", input_data=\"test1\", data_version=\"\",\n",
    "                                    feature_list=[\"Activity\", \"LapseTime\"],\n",
    "                                    encoding=encoding, label=\"Next_Activity\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "71d5045d-b455-4c8a-8f5f-001660baed1a",
   "metadata": {},
   "source": [
    "# Hyperparameters\n",
    "input_size = 7  # The number of expected features in the input x\n",
    "hidden_size = 512  # The number of features in the hidden state h\n",
    "num_layers = 1  # Number of recurrent layers\n",
    "num_classes = 6 \n",
    "learning_rate = 0.002\n",
    "\n",
    "\n",
    "model = DLModels.SimpleLSTM(input_size, hidden_size, num_layers, num_classes).to(torch_device)\n",
    "optimizer = torch.optim.NAdam(model.parameters(), lr=learning_rate)\n",
    "loss = nn.CrossEntropyLoss()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "286103af-95b6-46b5-bb8c-453d81e04c14",
   "metadata": {},
   "source": [
    "trained_model, train_score, test_score = train_model(model, optimizer, loss, loss, train, test,\n",
    "            100, torch_device, device_package, print_iter=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "22cd2eb5-45f3-4ca4-bc6b-e29c9f9f32dd",
   "metadata": {},
   "source": [
    "evaluate_model(trained_model, test, torch_device, device_package, batch_size=100)"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
