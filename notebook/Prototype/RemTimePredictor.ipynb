{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebfa4136-8b84-42a8-aaa4-88c32f0d48cd",
   "metadata": {},
   "source": [
    "# Remaining Time Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1cb3222-4188-4fd8-9ea0-88bc56fc25ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from joblib import dump, load\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import importlib.util\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from src.Trainer import CaseDataSet\n",
    "from src.Model import DLModels\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import f1_score\n",
    "    \n",
    "\n",
    "\n",
    "torch_device = \"cpu\"\n",
    "device_package = torch.cpu\n",
    "if importlib.util.find_spec(\"torch.backends.mps\") is not None:\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch_device = torch.device(\"mps\")\n",
    "        device_package = torch.mps\n",
    "if torch.cuda.is_available():\n",
    "    torch_device = torch.device(\"cuda\")\n",
    "    device_package = torch.cuda\n",
    "    \n",
    "torch_device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4823f08d-b679-4d93-841e-94b69f1e9559",
   "metadata": {},
   "source": [
    "## XGB remaining time predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34e1f7c9-a1b3-4682-a4c9-4b9f7e8f6cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBTrainer():\n",
    "    def __init__(self, training_set, validation_set, model_path, tree_method=\"hist\", early_stopping_rounds=2):\n",
    "        self.training_set = training_set\n",
    "        self.validation_set = validation_set\n",
    "        self.model_path = model_path\n",
    "        self.dataset_list = [self.training_set, self.validation_set]\n",
    "        self.data_list = []\n",
    "        self.reg = xgb.XGBRegressor(objective=\"reg:absoluteerror\", tree_method=tree_method, early_stopping_rounds=early_stopping_rounds)\n",
    "        self.generate_data_set()\n",
    "        \n",
    "    def generate_data_set(self):\n",
    "        for i in range(2):\n",
    "            feature_list = []\n",
    "            label_list = []\n",
    "            for prefix_len in range(1, self.dataset_list[i].max_case_len - 1):\n",
    "                self.dataset_list[i].set_prefix_length(prefix_len)\n",
    "                feature_list.append(self.dataset_list[i][:][0].numpy())\n",
    "                label_list.append(self.dataset_list[i][:][1].numpy())\n",
    "                #print(self.dataset_list[i][:][1].numpy())\n",
    "        \n",
    "            self.data_list.append([np.vstack(feature_list), np.vstack(label_list)])       \n",
    "    \n",
    "    def train(self):\n",
    "        self.reg.fit(self.data_list[0][0], self.data_list[0][1], eval_set=[(self.data_list[1][0], self.data_list[1][1])])\n",
    "        \n",
    "    def score(self):\n",
    "        return self.reg.score(self.data_list[1][0], self.data_list[1][1])\n",
    "    \n",
    "    def save_model(self):\n",
    "        self.clf.save_model(self.model_path + \"_XGB.json\")\n",
    "        dump(self.le, self.model_path + \"_le.joblib\")\n",
    "        \n",
    "    def load_model(self):\n",
    "        self.clf.load_model(self.model_path + \"_XGB.json\")\n",
    "        self.le = load(self.model_path + \"_le.joblib\")\n",
    "        \n",
    "\n",
    "class XGBPredictor():\n",
    "    def __init__(self, test_set, model_path, tree_method=\"hist\", early_stopping_rounds=2):\n",
    "        self.test_set = test_set\n",
    "        self.model_path = model_path\n",
    "        self.data_list = []\n",
    "        self.reg = xgb.XGBRegressor(objective=\"reg:absoluteerror\", tree_method=tree_method, early_stopping_rounds=early_stopping_rounds)\n",
    "        self.reg.load_model(self.model_path + \"_XGB.json\")\n",
    "        self.generate_data_set()\n",
    "\n",
    "    def generate_data_set(self):\n",
    "        feature_list = []\n",
    "        label_list = []\n",
    "        for prefix_len in range(1, self.test_set.max_case_len-1):\n",
    "            self.test_set.set_prefix_length(prefix_len)\n",
    "            feature_list.append(self.test_set[:][0].numpy())\n",
    "            label_list.append(self.test_set[:][1].numpy())\n",
    "\n",
    "        self.data_list = [np.vstack(feature_list), np.vstack(label_list)]\n",
    "        \n",
    "\n",
    "    def predict(self):\n",
    "        return self.reg.predict(self.data_list[0]), np.squeeze(self.data_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fe7f67d-c17c-44fe-a571-552561a43ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = \"Agg_Mean\"\n",
    "train = CaseDataSet.CaseDataset(project_data_path=\"../../data/\", input_data=\"t1\", data_version=\"_train\",\n",
    "                                    feature_list=[\"Activity\", \"LapseTime\"],\n",
    "                                    encoding=encoding, label=\"RemTime\")\n",
    "\n",
    "val = CaseDataSet.CaseDataset(project_data_path=\"../../data/\", input_data=\"t1\", data_version=\"_val\",\n",
    "                                    feature_list=[\"Activity\", \"LapseTime\"],\n",
    "                                    encoding=encoding, label=\"RemTime\")\n",
    "\n",
    "test = CaseDataSet.CaseDataset(project_data_path=\"../../data/\", input_data=\"t1\", data_version=\"_test\",\n",
    "                                    feature_list=[\"Activity\", \"LapseTime\"],\n",
    "                                    encoding=encoding, label=\"RemTime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d95e6e1a-ec7e-463e-a87c-586a4a259952",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = XGBTrainer(train, test, model_path=\"../../models/test_remtime\")\n",
    "t1.train()\n",
    "print(t1.score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3e1ee97-450c-4e6d-a416-5ae9d37549ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = XGBPredictor(test, model_path=\"../../models/test_remtime\")\n",
    "t2.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d406545-d3e8-4a6a-b61e-9841b5489f55",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e0ca4488-5aa3-46ba-be10-f2f71550cf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_epoch(model, training_set, optimizer, criterion, torch_device,\n",
    "                      batch_size=50, training=True):\n",
    "    training_data_set = training_set\n",
    "    batch_size = batch_size\n",
    "    loss_prefix_list = []\n",
    "    sample_num_list = []\n",
    "    for prefix_len in range(1, training_data_set.max_case_len - 1):\n",
    "        loss_prefix = 0\n",
    "        training_data_set.set_prefix_length(prefix_len)\n",
    "        training_data_set.shuffle_data()\n",
    "        input_data = training_data_set[:]\n",
    "        if input_data is None:\n",
    "            break\n",
    "        sample_num = input_data[0].shape[0]\n",
    "        sample_num_list.append(sample_num)\n",
    "\n",
    "        batch_num = int(sample_num / batch_size)\n",
    "        for i in range(batch_num):\n",
    "            x = input_data[0][int(batch_size * i) : int(batch_size * (i+1))].float().to(torch_device)\n",
    "            y = input_data[1][int(batch_size * i) : int(batch_size * (i+1))].float().to(torch_device)\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            if training:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            loss_prefix = loss_prefix + loss.item()\n",
    "\n",
    "        if sample_num > batch_size * batch_num:\n",
    "            x = input_data[0][batch_size * batch_num :].float().to(torch_device)\n",
    "            y = input_data[1][batch_size * batch_num :].float().to(torch_device)\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            if training:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            loss_prefix = loss_prefix + loss.item()\n",
    "\n",
    "        loss_prefix_list.append(loss_prefix)\n",
    "    return np.array(loss_prefix_list), np.array(sample_num_list)\n",
    "\n",
    "\n",
    "def train_model(model, optimizer, criterion, criterion_eval, training_set,\n",
    "                test_set, batch_size, torch_device, device_package,\n",
    "                max_epoch=100, max_ob_iter=20, score_margin=1e-4, print_iter=False):\n",
    "    train_score_list = []\n",
    "    test_score_list = []\n",
    "    score = 1e5\n",
    "    best_iter = 0\n",
    "    best_model = None\n",
    "    for iter_epoch in range(max_epoch):\n",
    "        device_package.empty_cache()\n",
    "        loss_train, sample_num_train = train_model_epoch(model, training_set, batch_size=batch_size,\n",
    "                                                                 optimizer=optimizer,\n",
    "                                                                 criterion=criterion,\n",
    "                                                                 torch_device=torch_device)\n",
    "        device_package.empty_cache()\n",
    "        loss_test, sample_num_test = train_model_epoch(model, test_set, batch_size=batch_size,\n",
    "                                                               optimizer=optimizer,\n",
    "                                                               criterion=criterion_eval,\n",
    "                                                               torch_device=torch_device,\n",
    "                                                               training=False)\n",
    "\n",
    "        score_train = np.sum(loss_train) / np.sum(sample_num_train)\n",
    "        score_test = np.sum(loss_test) / np.sum(sample_num_test)\n",
    "        train_score_list.append(score_train)\n",
    "        test_score_list.append(score_test)\n",
    "\n",
    "        if score_test < (score - score_margin):\n",
    "            score = score_test\n",
    "            best_model = copy.deepcopy(model)\n",
    "            best_iter = iter_epoch\n",
    "\n",
    "        if iter_epoch > best_iter + max_ob_iter:\n",
    "            break\n",
    "        if print_iter:\n",
    "            print(\"Finished training iteration: \", iter_epoch, \" with val loss: \", score_test)\n",
    "    device_package.empty_cache()\n",
    "    return best_model, np.array(train_score_list), np.array(test_score_list)\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_set, torch_device, device_package, batch_size=100):\n",
    "    training_data_set = test_set\n",
    "    evaluation_list = []\n",
    "    sample_num_list = []\n",
    "    model.flatten()\n",
    "    device_package.empty_cache()\n",
    "    for prefix_len in range(training_data_set.max_case_len - 1):\n",
    "        training_data_set.set_prefix_length(prefix_len + 1)\n",
    "        training_data_set.shuffle_data()\n",
    "        input_data = training_data_set[:]\n",
    "        if input_data is None:\n",
    "            # print(\"Max length reached, abort\")\n",
    "            break\n",
    "        sample_num = input_data[0].shape[0]\n",
    "        sample_num_list.append(sample_num)\n",
    "        \n",
    "        output_list = []\n",
    "        label_list = []\n",
    "        batch_num = int(sample_num / batch_size)\n",
    "        for i in range(batch_num):\n",
    "            x = input_data[0][int(batch_size * i) : int(batch_size * (i+1))].float().to(torch_device)\n",
    "            y = input_data[1][int(batch_size * i) : int(batch_size * (i+1))].float()\n",
    "            outputs = model(x).detach()\n",
    "            output_list.append(outputs.cpu().numpy())\n",
    "            label_list.append(y.cpu().numpy())\n",
    "            \n",
    "            device_package.empty_cache()\n",
    "\n",
    "        if sample_num > batch_size * batch_num:\n",
    "            x = input_data[0][batch_size * batch_num :].float().to(torch_device)\n",
    "            y = input_data[1][batch_size * batch_num :].float()\n",
    "            outputs = model(x).detach()\n",
    "            output_list.append(outputs.cpu().numpy())\n",
    "            label_list.append(y.cpu().numpy())\n",
    "            \n",
    "            device_package.empty_cache()\n",
    "        \n",
    "        evaluation_list.append([np.vstack(output_list),\n",
    "                                np.vstack(label_list)])\n",
    "        \n",
    "        \n",
    "    return evaluation_list, np.array(sample_num_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e390be6d-d05f-4fed-989a-68c1368e906b",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = \"All\"\n",
    "train = CaseDataSet.CaseDataset(project_data_path=\"../../data/\", input_data=\"test1\", data_version=\"\",\n",
    "                                    feature_list=[\"Activity\", \"LapseTime\"],\n",
    "                                    encoding=encoding, label=\"RemTime\")\n",
    "\n",
    "test = CaseDataSet.CaseDataset(project_data_path=\"../../data/\", input_data=\"test1\", data_version=\"\",\n",
    "                                    feature_list=[\"Activity\", \"LapseTime\"],\n",
    "                                    encoding=encoding, label=\"RemTime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5e068bc0-0be3-43d3-8c2f-c3e386729a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_size = 7  # The number of expected features in the input x\n",
    "hidden_size = 256  # The number of features in the hidden state h\n",
    "num_layers = 1  # Number of recurrent layers\n",
    "num_classes = 1 \n",
    "learning_rate = 0.001\n",
    "\n",
    "\n",
    "model = DLModels.SimpleLSTM(input_size, hidden_size, num_layers, num_classes).to(torch_device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8901bac3-f834-4064-a999-3774dfde6cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model, train_score, test_score = train_model(model, optimizer, loss, loss, train, test,\n",
    "            100, torch_device, device_package, print_iter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "80161c3a-2543-40b3-899b-8c93443ec9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(trained_model, test, torch_device, device_package, batch_size=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
